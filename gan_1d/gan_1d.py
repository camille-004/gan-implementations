"""
Simple GAN from scratch on a one-dimensional function
TensorFlow Implementation
"""
# %%
import os

import matplotlib.pyplot as plt
import numpy as np
from keras.layers import Dense, LeakyReLU
from keras.models import Sequential
from keras.utils.vis_utils import plot_model

LATENT_DIM = 5


def objective(x):
    """
    Example one-dimensional function from which to generate random samples

    :param x: Input x
    :return: float, f(x)
    """
    return np.sin(x)


def generate_real_samples(n=200):
    """
    Generate uniformly random values between -[range] and [range], then
    calculate objective function for each value. Return stack of inputs and
    outputs

    :param n: Number of samples to generate
    :return: np.array, stack of inputs and outputs
    """
    X_1 = np.random.uniform(-1.5, 1.5, n)  # generate random inputs
    X_2 = np.array([objective(x) for x in X_1])  # generate outputs
    X_1 = X_1.reshape(n, 1)
    X_2 = X_2.reshape(n, 1)
    X = np.hstack((X_1, X_2))
    y = np.ones((n, 1))
    return X, y


def generate_fake_samples(n):
    """
    Generate n fake examples. Return with class labels

    :param n: Number of samples to generate
    :return: np.array, stack of inputs and outputs
    """
    X_1 = np.random.uniform(-5, 5, n).reshape(n, 1)
    X_2 = np.random.uniform(-5, 5, n).reshape(n, 1)
    X = np.hstack((X_1, X_2))
    y = np.zeros((n, 1))
    return X, y


def build_discriminator(n_inputs=2):
    """
    Build a binary discriminator model

    :param n_inputs: Input dimension for model
    :return: Compiled discriminator model
    """
    model = Sequential()
    model.add(Dense(25,  kernel_initializer='he_uniform', input_dim=n_inputs))
    model.add(LeakyReLU(alpha=0.01))
    model.add(Dense(15,  kernel_initializer='he_uniform'))
    model.add(LeakyReLU(alpha=0.01))
    model.add(Dense(10,  kernel_initializer='he_uniform'))
    model.add(LeakyReLU(alpha=0.01))
    model.add(Dense(5,  kernel_initializer='he_uniform'))
    model.add(LeakyReLU(alpha=0.01))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(loss='binary_crossentropy',
                  optimizer='adam',
                  metrics=['accuracy'])
    return model


def train_discriminator(_D, n_epochs=1000, n_batches=128):
    """
    Train and evaluate discriminator model

    :param _D: Keras discriminator model
    :param n_epochs: Number of epochs for training
    :param n_batches: Number of batches for training
    :return:
    """
    # For each epoch, generate a half-batch of real and fake examples
    # Update the model on whole batch
    half_batch = int(n_batches / 2)

    for i in range(n_epochs):
        X_real, y_real = generate_real_samples(half_batch)
        X_fake, y_fake = generate_fake_samples(half_batch)

        # Update model
        _D.train_on_batch(X_real, y_real)
        _D.train_on_batch(X_fake, y_fake)

        # Evaluate model
        _, accuracy_real = _D.evaluate(X_real, y_real, verbose=0)
        _, accuracy_fake = _D.evaluate(X_fake, y_fake, verbose=0)

        print(f'Epoch {i}: real accuracy - {accuracy_real}, '
              f'fake accuracy - {accuracy_fake}')


def build_generator(latent_dim, n_outputs=2):
    """
    Build a generator model, which will be similar to the discriminator model.
    Inputs: Point in latent space (latent_dim-element vector of Gaussian
            random numbers)
            - Generated by drawing random numbers from standard Gaussian
              distribution
    Outputs: n_outputs-element vector containing generated sample (x and
             objective(x)), according to the objective function

    :param latent_dim: Size of latent space
    :param n_outputs: Number of elements in our output vector
    :return: Non-compiled generator model (as model won't be fit directly)
    """
    model = Sequential()
    model.add(Dense(15,
                    activation='relu',
                    kernel_initializer='he_uniform',
                    input_dim=latent_dim))

    # Linear activation because we want to output a real vector
    model.add(Dense(n_outputs, activation='linear'))

    return model


def generate_latent_points(latent_dim, n):
    """
    To train the generator, generate points in latent space (array of random
    numbers from a standard Gaussian distribution).

    :param latent_dim: Dimension of sample
    :param n: Number of samples
    :return: np.array (n x latent_dim), latent samples
    """
    X_input = np.random.randn(latent_dim * n)
    return X_input.reshape(n, latent_dim)


def generate_fake_samples_generator(_G, latent_dim, n, plot=False):
    """
    Generate fake examples with the generator, and plot them.

    :param _G: Generator model
    :param latent_dim: Size of latent space
    :param n: Number of samples to generate
    :param plot: Whether or not to plot latent generated points
    :return:
    """
    X_input = generate_latent_points(latent_dim, n)

    # Predict outputs (should ideally match objective function)
    X = _G.predict(X_input)

    if plot:
        plt.scatter(X[:, 0], X[:, 1])
        plt.show()

    y = np.zeros((n, 1))
    return X, y


def build_GAN(_G, _D):
    """
    Create subsumed GAN by stacking generator and discriminator, having their
    already-defined weights and layers. The discriminator does not need to be
    trained here.

    :param _G: Generator model, outputting a real vector
    :param _D: Discriminator model that will output probability of sample being
               real, only one integer
    :return: Composite GAN
    """
    _D.trainable = False

    # Stack generator and discriminator
    model = Sequential()
    model.add(_G)
    model.add(_D)

    model.compile(loss='binary_crossentropy', optimizer='adam')
    return model


def evaluate(epoch, _G, _D, latent_dim, n=200):
    """
    Evaluate the discriminator and plot generator examples at current epoch.

    :param epoch:  Current epoch
    :param _G: Generator model
    :param _D: Discriminator model
    :param latent_dim: Size of latent space
    :param n: Number of samples to generate
    :return:
    """
    X_real, y_real = generate_real_samples(n)
    _, accuracy_real = _D.evaluate(X_real, y_real, verbose=0)

    X_fake, y_fake = generate_fake_samples_generator(_G, latent_dim, n)
    _, accuracy_fake = _D.evaluate(X_fake, y_fake, verbose=0)

    print(f'Epoch {epoch}: real accuracy - {accuracy_real}, '
          f'fake accuracy - {accuracy_fake}')

    plt.scatter(X_real[:, 0], X_real[:, 1], color='red', label='real')
    plt.scatter(X_fake[:, 0], X_fake[:, 1], color='blue', label='generated')
    plt.title(f'Epoch {epoch}')
    plt.legend()
    plt.savefig(os.path.join('gan_1d/results/', f'epoch_{epoch}.png'))
    plt.show()


def train(_G, _D, _gan, latent_dim, n_epochs=10000, n_batches=16,
          n_eval=1000):
    """
    Train the generator and discriminator

    :param _G: Generator model
    :param _D: Discriminator model
    :param _gan: Subsumed GAN model
    :param latent_dim: Size of latent space
    :param n_epochs: Number of epochs for training
    :param n_batches: Number of batches for training
    :param n_eval: Each number of epochs at which to evaluate
    :return:
    """
    half_batch = int(n_batches / 2)

    for i in range(n_epochs):
        X_real, y_real = generate_real_samples(half_batch)
        X_fake, y_fake = generate_fake_samples_generator(
            _G, latent_dim, half_batch
        )

        # Update discriminator with real and fake samples
        _D.train_on_batch(X_real, y_real)
        _D.train_on_batch(X_fake, y_fake)

        # Prepare points in latent space for generator
        X_gan = generate_latent_points(latent_dim, n_batches)
        y_gan = np.ones((n_batches, 1))  # Inverted labels for fake samples

        # Update generator with discriminator's error
        _gan.train_on_batch(X_gan, y_gan)

        if (i + 1) % n_eval == 0:
            evaluate(i + 1, _G, _D, latent_dim)


if __name__ == '__main__':
    D = build_discriminator()
    G = build_generator(LATENT_DIM)
    gan = build_GAN(G, D)
    gan.summary()
    plot_model(gan,
               to_file='gan_1d/gan_plot.png',
               show_shapes=True,
               show_layer_names=True)
    train(G, D, gan, LATENT_DIM, n_epochs=10000, n_batches=256)
